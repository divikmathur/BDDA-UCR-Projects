{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BDDA Assignment question 1\n",
    "\n",
    "Divik Mathur; 015031\n",
    "BDA-01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification of Corona Tweets - NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Spark Session\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing the TextClassifier app\n",
    "spark= SparkSession.builder.appName('TextClassifierApp').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading file\n",
    "df = spark.read.csv('Corona_NLP_train.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+---------+\n",
      "|            UserName|          ScreenName|            Location|             TweetAt|       OriginalTweet|Sentiment|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+---------+\n",
      "|                3799|               48751|              London|          16-03-2020|@MeNyrbie @Phil_G...|  Neutral|\n",
      "|                3800|               48752|                  UK|          16-03-2020|advice Talk to yo...| Positive|\n",
      "|                3801|               48753|           Vagabonds|          16-03-2020|Coronavirus Austr...| Positive|\n",
      "|                3802|               48754|                null|          16-03-2020|My food stock is ...|     null|\n",
      "|              PLEASE|         don't panic| THERE WILL BE EN...|                null|                null|     null|\n",
      "|           Stay calm|          stay safe.|                null|                null|                null|     null|\n",
      "|#COVID19france #C...|            Positive|                null|                null|                null|     null|\n",
      "|                3803|               48755|                null|          16-03-2020|Me, ready to go a...|     null|\n",
      "|Not because I'm p...| but because my f...|          but please| don't panic. It ...|                null|     null|\n",
      "|#CoronavirusFranc...|  Extremely Negative|                null|                null|                null|     null|\n",
      "|                3804|               48756|ÜT: 36.319708,-82...|          16-03-2020|As news of the re...| Positive|\n",
      "|                3805|               48757|35.926541,-78.753267|          16-03-2020|\"Cashier at groce...| Positive|\n",
      "|                3806|               48758|             Austria|          16-03-2020|Was at the superm...|     null|\n",
      "|#toiletpapercrisi...|             Neutral|                null|                null|                null|     null|\n",
      "|                3807|               48759|     Atlanta, GA USA|          16-03-2020|Due to COVID-19 o...| Positive|\n",
      "|                3808|               48760|    BHAVNAGAR,GUJRAT|          16-03-2020|For corona preven...| Negative|\n",
      "|                3809|               48761|      Makati, Manila|          16-03-2020|All month there h...|  Neutral|\n",
      "|                3810|               48762|Pitt Meadows, BC,...|          16-03-2020|Due to the Covid-...|     null|\n",
      "|The wait time may...| particularly bee...|                null|                null|                null|     null|\n",
      "|We thank you for ...|  Extremely Positive|                null|                null|                null|     null|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['UserName', 'ScreenName', 'Location', 'TweetAt', 'OriginalTweet', 'Sentiment']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- UserName: string (nullable = true)\n",
      " |-- ScreenName: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- TweetAt: string (nullable = true)\n",
      " |-- OriginalTweet: string (nullable = true)\n",
      " |-- Sentiment: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65274\n"
     ]
    }
   ],
   "source": [
    "#dropping the duplicate values\n",
    "df = df.dropDuplicates()\n",
    "print(df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36657"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.toPandas()['Sentiment'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=('Sentiment'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering out data in the dataset\n",
    "df = df.filter(df.Sentiment.isin(sentiments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the list of different sentiments\n",
    "sentiments = ['Positive','Negative','Neutral','Extremely Positive','Extremely Negative']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computes the character length of string data\n",
    "from pyspark.sql.functions import length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Viewing the length of each and every tweet\n",
    "df=df.withColumn('length', length(df['OriginalTweet']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-------------------+----------+--------------------+------------------+------+\n",
      "|UserName|ScreenName|           Location|   TweetAt|       OriginalTweet|         Sentiment|length|\n",
      "+--------+----------+-------------------+----------+--------------------+------------------+------+\n",
      "|    4754|     49706|     In the kitchen|17-03-2020|Going to the groc...|Extremely Positive|   182|\n",
      "|    4894|     49846|                 ??|17-03-2020|Two grocery store...|          Positive|   185|\n",
      "|    4927|     49879|     Washington, DC|17-03-2020|For more updates,...|           Neutral|    71|\n",
      "|    5331|     50283|    London, England|17-03-2020|I think peoples g...|Extremely Negative|   258|\n",
      "|    5674|     50626|     Pozna?, Polska|17-03-2020|Not only does uni...|Extremely Positive|   240|\n",
      "|    6238|     51190|                 DC|17-03-2020|Any news on mortg...|          Positive|   199|\n",
      "|    6392|     51344|               null|17-03-2020|@agirlmegan Know ...|          Negative|   255|\n",
      "|    6494|     51446|    Los Angeles, CA|18-03-2020|The city of Los A...|           Neutral|   120|\n",
      "|    6621|     51573|         Texas, USA|18-03-2020|As the world deal...|          Positive|   243|\n",
      "|    6883|     51835|         FL, U.S.A.|18-03-2020|I can tell this w...|          Positive|   168|\n",
      "|    7450|     52402|               null|18-03-2020|COVID-19 panic bu...|          Negative|   171|\n",
      "|    7840|     52792|       Minneapolis |18-03-2020|Do grocery stores...|           Neutral|   150|\n",
      "|    8195|     53147|       South Africa|18-03-2020|@boy_mathaithai @...|Extremely Negative|   176|\n",
      "|    8247|     53199|        Phoenix, AZ|18-03-2020|Dear person who a...|Extremely Positive|   185|\n",
      "|    8344|     53296|                 VA|18-03-2020|Sad sign of the t...|          Positive|   217|\n",
      "|    8496|     53448|      Charlotte, NC|18-03-2020|I bought a turkey...|Extremely Positive|   255|\n",
      "|    8610|     53562|    Los Angeles, CA|18-03-2020|Today I saw a man...|           Neutral|   147|\n",
      "|    8932|     53884|Huntsville, Alabama|18-03-2020|They dont need ...|           Neutral|    76|\n",
      "|    9239|     54191|               null|19-03-2020|All payments on m...|          Negative|   216|\n",
      "|    9347|     54299|    New Jersey, USA|19-03-2020|my mother is a re...|          Negative|   278|\n",
      "+--------+----------+-------------------+----------+--------------------+------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Viewing the length of each and every tweet\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#renaming the column\n",
    "df=df.withColumnRenamed(\"Sentiment\",\"sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+\n",
      "|         Sentiment|       avg(length)|\n",
      "+------------------+------------------+\n",
      "|Extremely Negative| 209.6656891495601|\n",
      "|          Positive|193.66195905675045|\n",
      "|           Neutral| 151.2949846860643|\n",
      "|          Negative| 189.6651596908269|\n",
      "|Extremely Positive| 215.0605167724388|\n",
      "+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#calculating the mean length of different sentiments\n",
    "df.groupby('Sentiment').mean().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performing Tokeization\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF, StringIndexer\n",
    "\n",
    "tokenizer=Tokenizer(inputCol=\"OriginalTweet\", outputCol=\"token_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing Stopwords from the comments using count vectorizer, inverse document frequency and string indexer to transform the text\n",
    "stopremove=StopWordsRemover(inputCol=\"token_text\", outputCol=\"stop_tokens\")\n",
    "count_vec=CountVectorizer(inputCol=\"stop_tokens\", outputCol=\"c_vec\")\n",
    "idf=IDF(inputCol=\"c_vec\", outputCol=\"tf_idf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the labels in numbers\n",
    "label_to_num = StringIndexer(inputCol=\"sentiment\", outputCol='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature transformer that merges multiple columns into a vector column.\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = VectorAssembler(inputCols=['tf_idf','length'], outputCol='features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "#instantitating decision tree classifier\n",
    "\n",
    "dtc=DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "#building a pieline for pre-processing the text \n",
    "\n",
    "pipeline= Pipeline(stages=[label_to_num, tokenizer, stopremove, count_vec, idf, cleaned])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitting the model\n",
    "\n",
    "cleaned_data = pipeline.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transforming the data\n",
    "\n",
    "cleaned_data = cleaned_data.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting labels and features \n",
    "\n",
    "cleaned_data = cleaned_data.select(['label', 'features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  3.0|(78306,[0,3,6,12,...|\n",
      "|  0.0|(78306,[0,3,6,51,...|\n",
      "|  2.0|(78306,[398,10236...|\n",
      "|  4.0|(78306,[4,7,33,34...|\n",
      "|  3.0|(78306,[7,10,36,5...|\n",
      "|  0.0|(78306,[0,1,10,23...|\n",
      "|  1.0|(78306,[1,5,8,21,...|\n",
      "|  2.0|(78306,[3,6,88,99...|\n",
      "|  0.0|(78306,[1,7,9,11,...|\n",
      "|  0.0|(78306,[13,27,48,...|\n",
      "|  1.0|(78306,[8,21,28,3...|\n",
      "|  2.0|(78306,[0,3,57,71...|\n",
      "|  4.0|(78306,[4,8,17,21...|\n",
      "|  3.0|(78306,[0,7,17,19...|\n",
      "|  0.0|(78306,[0,3,6,70,...|\n",
      "|  3.0|(78306,[0,3,6,16,...|\n",
      "|  2.0|(78306,[0,3,6,72,...|\n",
      "|  2.0|(78306,[16,17,117...|\n",
      "|  1.0|(78306,[10,96,98,...|\n",
      "|  1.0|(78306,[5,7,9,11,...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#viewing the selected columns\n",
    "\n",
    "cleaned_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spilting the data into train and test\n",
    "\n",
    "(training, testing)=cleaned_data.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitting the model on the dataset\n",
    "\n",
    "dtpredictor = dtc.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transforming the model\n",
    "\n",
    "test_results = dtpredictor.transform(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|  0.0|(78306,[0,1,5,7,1...|[3008.0,2728.0,13...|[0.28710508733416...|       0.0|\n",
      "|  0.0|(78306,[0,3,6,51,...|[3008.0,2728.0,13...|[0.28710508733416...|       0.0|\n",
      "|  0.0|(78306,[0,4,16,31...|[186.0,51.0,8.0,2...|[0.37575757575757...|       3.0|\n",
      "|  0.0|(78306,[0,18,51,5...|[3008.0,2728.0,13...|[0.28710508733416...|       0.0|\n",
      "|  0.0|(78306,[2,18,92,2...|[3008.0,2728.0,13...|[0.28710508733416...|       0.0|\n",
      "|  0.0|(78306,[5,16,19,1...|[3008.0,2728.0,13...|[0.28710508733416...|       0.0|\n",
      "|  0.0|(78306,[5,16,50,1...|[3008.0,2728.0,13...|[0.28710508733416...|       0.0|\n",
      "|  0.0|(78306,[36,102,13...|[3008.0,2728.0,13...|[0.28710508733416...|       0.0|\n",
      "|  0.0|(78306,[45,262,26...|[1521.0,1356.0,22...|[0.25575920632251...|       2.0|\n",
      "|  1.0|(78306,[0,1,12,21...|[84.0,204.0,16.0,...|[0.13636363636363...|       4.0|\n",
      "|  1.0|(78306,[0,7,13,16...|[84.0,204.0,16.0,...|[0.13636363636363...|       4.0|\n",
      "|  1.0|(78306,[0,15,26,6...|[3008.0,2728.0,13...|[0.28710508733416...|       0.0|\n",
      "|  1.0|(78306,[2,8,489,1...|[1521.0,1356.0,22...|[0.25575920632251...|       2.0|\n",
      "|  1.0|(78306,[2,9,24,10...|[73.0,130.0,6.0,1...|[0.30543933054393...|       1.0|\n",
      "|  1.0|(78306,[2,46,71,2...|[1521.0,1356.0,22...|[0.25575920632251...|       2.0|\n",
      "|  1.0|(78306,[4,9,21,45...|[84.0,204.0,16.0,...|[0.13636363636363...|       4.0|\n",
      "|  1.0|(78306,[5,8,68,23...|[3008.0,2728.0,13...|[0.28710508733416...|       0.0|\n",
      "|  1.0|(78306,[5,29,43,1...|[1521.0,1356.0,22...|[0.25575920632251...|       2.0|\n",
      "|  1.0|(78306,[5,32,73,1...|[1521.0,1356.0,22...|[0.25575920632251...|       2.0|\n",
      "|  1.0|(78306,[7,21,34,4...|[18.0,72.0,2.0,6....|[0.09677419354838...|       4.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#viewing the results of the test data\n",
    "test_results.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Evaluator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiating the classfication evaluator\n",
    "acc_eval=MulticlassClassificationEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluating the accuracy of the model\n",
    "acc=acc_eval.evaluate(test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model is:: 0.2921386154999538\n"
     ]
    }
   ],
   "source": [
    "print (\"Accuracy of the model is::\", acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
